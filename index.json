
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m a PhD student from Unmanned Systems Research Group, Department of Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, supervised by Prof. Ben M. Chen.\nPrior to that, I received my bachelor’s and master’s degrees from School of Geodesy and Geomatics, Wuhan University, being advised by Prof. Xingxing Li.\n","date":1720137600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720137600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m a PhD student from Unmanned Systems Research Group, Department of Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, supervised by Prof. Ben M. Chen.\nPrior to that, I received my bachelor’s and master’s degrees from School of Geodesy and Geomatics, Wuhan University, being advised by Prof.","tags":null,"title":"Zongzhou Wu","type":"authors"},{"authors":["Zhiheng Shen","Xingxing Li","Yuxuan Zhou","Shengyu Li","Zongzhou Wu","Xuanbin Wang"],"categories":null,"content":" ","date":1720137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720137600,"objectID":"5cf33a163e276ef1f8b25b2fce572713","permalink":"https://zzwu29.github.io/publication/journal-article/tase-1/","publishdate":"2024-07-05T00:00:00Z","relpermalink":"/publication/journal-article/tase-1/","section":"publication","summary":"Continuous and reliable estimation of navigation states is of paramount importance in ensuring the safe operation of intelligent vehicles. The conventional global navigation satellite system (GNSS)-Inertial-Visual navigation systems have demonstrated the capability to achieve locally accurate and globally drift-free pose estimation. However, challenges such as frequent satellite signal interference, limited visual features, and sudden sensor failures can severely degrade performance or even lead to complete system collapse when utilizing minimal sensor configurations. For this reason, we propose a novel and globally drift-free tightly coupled (TC) system that can integrate any number of GNSS, inertial measurement units (IMUs), and cameras to enable accurate and robust vehicle navigation. Specifically, a stacked state estimator centered on the IMU is designed to fuse information from all sensors at the raw measurement level. The pseudorange and carrier phase measurements from all GNSS terminals are directly correlated with the core IMU, ensuring accurate and fast positioning of the system in a global frame. The feature measurements from multiple independent cameras are also used to update the states by exploiting inter-epoch geometric constraints. In addition, the multiple homogeneous IMUs can not only further improve the state estimation of the system by imposing rigid constraints on the core IMU, but also switch over in time for smooth state estimation when the core IMU are faulty. We comprehensively evaluate the state estimation accuracy and robustness of the proposed approach through a series of in-vehicle experiments and simulation experiments in real urban scenarios. The results indicate that the proposed system can achieve 93.2% availability with a horizontal position error less than 0.5 m and 97.8% availability with a heading error less than 0.2 deg in typical urban environments, significantly outperforming both conventional and state-of-the-art approaches. Note to Practitioners —This study focuses on the tight integration of multiple homogeneous and heterogeneous sensors with the goal of addressing frequent interference and degradation challenges in wide-area vehicle navigation applications. We propose a general-purpose GNSS-Inertial-Visual tight coupled framework capable of integrating any number of GNSS, IMUs, and cameras at the raw measurement level. It maximizes the use of as much sensor information as possible to achieve accurate and robust state estimation and is resilient to anomalous measurements and sensor unavailability. This solution holds practical and effective for autonomous vehicles that are now commonly equipped with multiple sensors.","tags":["GNSS","sensor fusion"],"title":"Accurate and capable GNSS-inertial-visual vehicle navigation via tightly coupled multiple homogeneous sensors","type":"publication"},{"authors":["Zongzhou Wu","Xingxing Li","Zhiheng Shen","Zhili Xu","Shengyu Li","Yuxuan Zhou","Xin Li"],"categories":null,"content":" ","date":1716940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716940800,"objectID":"b07ed0faad3a3b310cfaeca96faa49ca","permalink":"https://zzwu29.github.io/publication/journal-article/tim-2/","publishdate":"2024-05-29T00:00:00Z","relpermalink":"/publication/journal-article/tim-2/","section":"publication","summary":"Accurate ego-motion estimation is the foundation for autonomous vehicle navigation in complex urban environments. In multisensor fusion schemes, in addition to environment-related degradations, sensor measurements sometimes fail, which may have an unpredictable impact on navigation performance. To make the vehicle sensor system resistant to different types of failures, we propose a failure-resistant and lightweight multisensor fusion method, in which pseudorange and carrier phase from multiple global navigation satellite system (GNSS) constellations, sparse visual features from a monocular camera, and records from multiple low-cost micro-electromechanical system (MEMS) inertial measurement units (IMUs) are integrated at the measurement level by a sliding-window tightly coupled filter. Furthermore, considering the possibilities of the high-rate IMUs recovering from failures, we propose a flexible strategy to fully utilize all available IMU measurements without frequent reinitializations. To verify the proposed method, we perform extensive evaluations under different sensor failures (e.g., GNSS short-term blocking and long-term failures, image failures, and complex IMU failures and recoveries). The results show that our method outperforms conventional approaches that include only a single IMU and merely focus on sensor degradation, and resists various types of sensor failures. Despite initializing under a harsh environment and experiencing complex sensor failures, velocity and yaw estimations are significantly improved while a submeter 3-D positioning can be achieved in approximately 80% of the challenging scenario. For complex IMU failure and recovery records, our method can fully utilize all available IMU measurements and seamlessly switch between the base and auxiliary IMUs to recover trajectories that approximate the ground truth.","tags":["GNSS","sensor fusion"],"title":"A failure-resistant, lightweight, and tightly coupled GNSS/INS/vision vehicle integration for complex urban environments","type":"publication"},{"authors":["Zhiheng Shen","Xingxing Li","Xuanbin Wang","Zongzhou Wu","Xin Li","Yuxuan Zhou","Shengyu Li"],"categories":null,"content":" ","date":1713830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713830400,"objectID":"b8f5fb00d9c88e5b757f39074658c604","permalink":"https://zzwu29.github.io/publication/journal-article/tits-1/","publishdate":"2024-04-23T00:00:00Z","relpermalink":"/publication/journal-article/tits-1/","section":"publication","summary":"Accurate position, velocity and orientation are essential for the autonomous navigation of unmanned vehicles. The integration of GNSS and INS that can deliver continuous navigation states is widely used for intelligent vehicle systems. In this paper, we propose a tightly coupled GNSS/INS positioning framework with carrier-phase ambiguity resolution based on factor graph optimization (FGO). In this approach, a sliding window optimizer is employed to fuse the multi-GNSS pseudorange and carrier-phase observations with inertial measurements. The same ambiguity within the window is considered as a state node, and the constraint on the ambiguity is continuously preserved by marginalization. To further improve the accuracy and reliability of precise positioning, the carrier-phase ambiguity resolution is introduced to the FGO-based GNSS/INS framework. When the vehicle is detected to be stationary, a zero-velocity constraint and an attitude invariant constraint will be imposed. Several experimental results indicate that the proposed method can accomplish the centimeter-level position estimation performance with beyond 90% positioning availability (horizontal \u003c 10 cm and vertical \u003c 10 cm) and outperforms the current state-of-the-art filter-based tightly coupled method.","tags":["GNSS","sensor fusion"],"title":"A novel factor graph framework for tightly coupled GNSS/INS integration with carrier-phase ambiguity resolution","type":"publication"},{"authors":null,"categories":null,"content":" ","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"f4c5e63031d9359650b53d96f26d9600","permalink":"https://zzwu29.github.io/project/traj_eval/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/project/traj_eval/","section":"project","summary":"Scripts to quickly calculate absolute/relative pose estimation error","tags":["Tool"],"title":"traj_eval","type":"project"},{"authors":["Zhiheng Shen","Xingxing Li","Xin Li","Zhili Xu","Zongzhou Wu","Yuxuan Zhou"],"categories":null,"content":" ","date":1701216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701216000,"objectID":"9ff613aedd22330ed51fcf79c10d9270","permalink":"https://zzwu29.github.io/publication/journal-article/tim-1/","publishdate":"2023-11-29T00:00:00Z","relpermalink":"/publication/journal-article/tim-1/","section":"publication","summary":"In the realm of signal interference mitigation and precise attitude estimation, the utilization of multiple homogeneous Global Navigation Satellite System (GNSS) antennas/ terminals has demonstrated notable advantages. However, the potential benefits of homogeneous sensor fusion for enhancing position estimation have been relatively unexplored. In this paper, we propose a tightly coupled precise point positioning (PPP)/INS vehicle navigation algorithm without base stations, which can integrate an arbitrary number of homogeneous GNSS terminals with the aim of improving position estimation and speeding up system recovery from interference. Specifically, for the measurement representation, the original pseudorange and carrier-phase observations from all GNSS terminals are tightly incorporated with the IMU data to enable optimal state estimation. On the other hand, we exploit the known geometry between terminals and the spatially atmospheric correlation of observations to compress the states, thus to accelerate system convergence. Furthermore interestingly, since each GNSS terminal is peer-to-peer rather than master-slave in the proposed IMU-centric system, it enables a seamless operation without switching even in the event of an unexpected GNSS terminal failure. Real-world experiments have demonstrated that the proposed system can achieve a root mean square error (RMSE) of approximately 0.3 m for horizontal position estimation, and 92.24% availability at a 0.5 m boundary, outperforming the prevailing methods in terms of position estimation, state convergence and performance in extreme scenarios.","tags":["GNSS","sensor fusion"],"title":"Precise and robust IMU-centric vehicle navigation via tightly integrating multiple homogeneous GNSS terminals","type":"publication"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":"CMake You should include python directory in CMakeLists.txt as\nfind_path(Third_python_ROOT HINTS \u0026#34;${Third_python_ROOT}\u0026#34; \u0026#34;$ENV{Third_python_ROOT} \u0026#34;) ...... include_directories(${Third_python_ROOT}/include) ...... target_link_libraries(${PROJECT_NAME} ${Third_python_ROOT}/libs/python39.lib) # python 3.9 as an example Modify .h and .cpp file You should include python header file in your header file as\n#include \u0026lt;Python.h\u0026gt; and here is an example of init\n// init Py_Initialize(); if (!Py_IsInitialized()) return; // an example to run python command as a string PyRun_SimpleString(\u0026#34;import sys\u0026#34;); // an example to set python parameters pyParams = PyDict_New(); //mind: Py_BuildValue \u0026#39;s\u0026#39; will automatically add \u0026#34; at beginning and end, so \u0026#34;\\\u0026#34;\u0026#34; + ... + \u0026#34;\\\u0026#34;\u0026#34; will result wrong !!! PyDict_SetItemString(pyParams, \u0026#34;weights_path\u0026#34;, Py_BuildValue(\u0026#34;s\u0026#34;, string(weights_path).c_str())); //string PyDict_SetItemString(pyParams, \u0026#34;H\u0026#34;, Py_BuildValue(\u0026#34;i\u0026#34;, H)); //int PyDict_SetItemString(pyParams, \u0026#34;conf_thresh\u0026#34;, Py_BuildValue(\u0026#34;d\u0026#34;, conf_thresh)); //double or float PyDict_SetItemString(pyParams, \u0026#34;cuda\u0026#34;, Py_True); //bool // init threads PyEval_InitThreads(); PyEval_ReleaseThread(PyThreadState_Get()); and here is an example of run a function\nPyGILState_STATE state; state = PyGILState_Ensure(); pModule = PyImport_ImportModule(\u0026#34;...\u0026#34;); // ... is python file name pFunc = PyObject_GetAttrString(pModule, \u0026#34;......\u0026#34;); // ...... is the function name in file ... if (pModule \u0026amp;\u0026amp; PyCallable_Check(pFunc)) { PyObject* pArgs = PyTuple_New(1); PyTuple_SetItem(pArgs, 0, pyParams); PyObject* pReturn = PyEval_CallObject(pFunc, pArgs); if (pReturn) { PyArrayObject* fea = (PyArrayObject*)pReturn; // ---------- add here ---------- Py_DECREF(fea); } else { cout \u0026lt;\u0026lt; \u0026#34;python not valid return! error!\u0026#34; \u0026lt;\u0026lt; endl; } } and you should close the python module once you do not need\nPy_Finalize(); Interact with numpy I/O parameters You should include numpy directory in CMakeLists.txt as\ninclude_directories(${Third_python_ROOT}/Lib/site-packages/numpy/core/include) You should include numpy header file in your header file as\n#include \u0026lt;numpy/arrayobject.h\u0026gt; An example to receive the numpy return\nif (pModule \u0026amp;\u0026amp; PyCallable_Check(pFunc)) { PyObject* pArgs = PyTuple_New(1); PyTuple_SetItem(pArgs, 0, pyParams); PyObject* pReturn = PyEval_CallObject(pFunc, pArgs); if (pReturn) { PyArrayObject* fea = (PyArrayObject*)pReturn; // -------------------- int fea_row = fea-\u0026gt;dimensions[0], fea_col = fea-\u0026gt;dimensions[1]; for (int idx_col = 0; idx_col \u0026lt; fea_col; idx_col++) { double x = 0.0, y = 0.0, z = 0; for (int idx_row = 0; idx_row \u0026lt; 2; idx_row++) { if (idx_row == 0) { x = *(double*)(fea-\u0026gt;data + idx_row * fea-\u0026gt;strides[0] + idx_col * fea-\u0026gt;strides[1]); } if (idx_row == 1) { y = *(double*)(fea-\u0026gt;data + idx_row * fea-\u0026gt;strides[0] + idx_col * fea-\u0026gt;strides[1]); z = *(double*)(fea-\u0026gt;data + 2 * fea-\u0026gt;strides[0] + idx_col * fea-\u0026gt;strides[1]); n_pts.push_back(cv::Point2f(x, y)); add_feature_num = add_feature_num + 1; } } } // -------------------- Py_DECREF(fea); } else { cout \u0026lt;\u0026lt; \u0026#34;python not valid return! error!\u0026#34; \u0026lt;\u0026lt; endl; } } Reference Link https://blog.csdn.net/qq_25005909/article/details/79092315\n","date":1699142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"b952657ed4f40480f548cb57e7587a74","permalink":"https://zzwu29.github.io/post/integrate-cpp-python/","publishdate":"2023-11-05T00:00:00Z","relpermalink":"/post/integrate-cpp-python/","section":"post","summary":"Hybrid Programming","tags":["tool","programming"],"title":"Integrate C++ and Python in programming","type":"post"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":"Project https://github.com/zzwu29/SuperPointCPP\n","date":1697068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697068800,"objectID":"af8e7e8839d3d2ba97e5897a1bdb1ee0","permalink":"https://zzwu29.github.io/post/superpointcpp/","publishdate":"2023-10-12T00:00:00Z","relpermalink":"/post/superpointcpp/","section":"post","summary":"Superpoint C++ function","tags":["computer vision","feature detector"],"title":"SuperPointCPP","type":"post"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":"Introduction When I tried to solve a state estimation via visual-inertial odometry, I found that the some photos captured by camera were lost.\nOpen-source Project https://film-net.github.io/ https://github.com/google-research/frame-interpolation Paper https://arxiv.org/pdf/2202.04901\nScripts Download\nProblem significant linear motion turn a corner frame interval too long (\u0026gt;= 0.2s in autonomous driving) ","date":1696982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696982400,"objectID":"2d4abfbb3ad7fec031fb12412732fca1","permalink":"https://zzwu29.github.io/post/frame-interpolation/","publishdate":"2023-10-11T00:00:00Z","relpermalink":"/post/frame-interpolation/","section":"post","summary":"interpolation of images and videos","tags":["computer vision"],"title":"Frame Interpolation","type":"post"},{"authors":null,"categories":null,"content":" ","date":1694995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694995200,"objectID":"93ef502defab0071010dd4352b060f8d","permalink":"https://zzwu29.github.io/project/superpointcpp/","publishdate":"2023-09-18T00:00:00Z","relpermalink":"/project/superpointcpp/","section":"project","summary":"An implement of SuperPoint feature detector in C++","tags":["Tool"],"title":"SuperPointCPP","type":"project"},{"authors":["Xingxing Li","Zongzhou Wu","Zhiheng Shen","Zhili Xu","Xin Li","Shengyu Li","Junjie Han"],"categories":null,"content":" ","date":1693958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693958400,"objectID":"e2440e551f719d64fbb8fa3d1a240bf0","permalink":"https://zzwu29.github.io/publication/journal-article/jsen-1/","publishdate":"2023-10-16T00:00:00Z","relpermalink":"/publication/journal-article/jsen-1/","section":"publication","summary":"Seamless positioning is critical for mobile robots to achieve ubiquitous availability in all scenarios. The precise point positioning (PPP) technique provides a centimeter-level solution without a reference station and is often integrated with an inertial navigation system (INS) for outdoor navigation of unmanned ground vehicles (UGVs). However, in challenging environments such as indoors, the performance of PPP/INS systems is severely degraded, while ultrawideband (UWB) is widely used due to its high accuracy over short distances. This article proposes a tightly coupled (TC) PPP/INS/UWB integrated system, in which satellite-difference ionosphere-free (SD-IF) pseudorange and carrier-phase measurements from a single receiver, microelectromechanical system (MEMS) inertial measurements, and the UWB rangings are tightly fused to accomplish continuous and precise positioning throughout indoor and outdoor coverage. To mitigate the effects of nonline-of-sight (NLOS), multipath, and faulty signals, a two-step weighting approach is proposed to improve UWB positioning performance, including segmentation weighting based on the signal strength, and weight adjustment based on area identification. Besides, motion constraints are imposed on velocity, yaw, and height to suppress drifts during signal-harsh periods. Real-world experiment results show that the proposed system can achieve continuous positioning in the overall indoor and outdoor environments, with the mean absolute errors (MAEs) of (0.214, 0.186, and 0.243 m) and the root mean square errors (RMSEs) of (0.339, 0.291, and 0.414 m) in the east, north, and up directions. In transitional areas with severe signal interference and interruption, a 0.5 m positioning accuracy and smooth switching are still attainable.","tags":["seamless positioning","sensor fusion"],"title":"An indoor and outdoor seamless positioning system for low-cost UGV using PPP/INS/UWB tightly coupled integration","type":"publication"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":"Title An indoor and outdoor seamless positioning system for low-cost UGV using PPP/INS/UWB tightly coupled integration\nPublished in: IEEE Sensors Journal\nAuthor Xingxing Li, Zongzhou Wu, Zhiheng Shen, Zhili Xu, Xin Li, Shengyu Li and Junjie Han\nAbstract Seamless positioning is critical for mobile robots to achieve ubiquitous availability in all scenarios. Precise point positioning (PPP) technique provides a centimeter-level solution without a reference station and is often integrated with an inertial navigation system (INS) for outdoor navigation of unmanned ground vehicles (UGVs). However, in challenging environments such as indoors, the performance of PPP/INS systems is severely degraded, while ultra-wideband (UWB) is widely used due to its high accuracy over short distances. This paper proposes a tightly coupled PPP/INS/UWB integrated system, in which satellite-difference ionosphere-free pseudorange and carrier-phase measurements from a single receiver, micro-electro-mechanical system (MEMS) inertial measurements, and the UWB rangings are tightly fused to accomplish continuous and precise positioning throughout indoor and outdoor coverage. To mitigate the effects of non-line-of-sight (NLOS), multipath and faulty signals, a two-step weighting approach is proposed to improve UWB positioning performance, including segmentation weighting based on the signal strength, and weight adjustment based on area identification. Besides, motion constraints are imposed on velocity, yaw, and height to suppress drifts during signal-harsh periods. Real-world experiment results show that the proposed system can achieve continuous positioning in the overall indoor and outdoor environments, with the MAEs of (0.214 m, 0.186 m, 0.243 m) and the RMSEs of (0.339 m, 0.291 m, 0.414 m) in the east, north, and up directions. In transitional areas with severe signal interference and interruption, a 0.5 m positioning accuracy and smooth switching are still attainable.\nKeyword Area identification, micro-electro-mechanical system, multisensor fusion, precise point positioning, seamless positioning, tightly coupled integration, ultra-wideband\nDOI 10.1109/JSEN.2023.3310480\nLink https://ieeexplore.ieee.org/document/10242329\n","date":1693180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693180800,"objectID":"14bb76616d48f8d19a17a7e55d9dda6f","permalink":"https://zzwu29.github.io/post/jsen-1/","publishdate":"2023-08-28T00:00:00Z","relpermalink":"/post/jsen-1/","section":"post","summary":"PPP/INS/UWB tightly coupled integration","tags":["seamless positioning","sensor fusion"],"title":"A work on indoor-outdoor positioning","type":"post"},{"authors":["Xingxing Li","Junjie Han","Xin Li","Jiaxin Huang","Zhiheng Shen","Zongzhou Wu"],"categories":null,"content":" ","date":1692835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692835200,"objectID":"a6034111b3420183db9a67f505b73d20","permalink":"https://zzwu29.github.io/publication/journal-article/gpss-1/","publishdate":"2023-08-24T00:00:00Z","relpermalink":"/publication/journal-article/gpss-1/","section":"publication","summary":"The performance of precise point positioning real-time kinematic (PPP-RTK) is closely tied to the accuracy of atmospheric corrections, with the ionospheric delay, including its uncertainty, being of particular importance. In this study, a grid-based slant ionospheric weighted method is proposed to enhance PPP-RTK performance across diverse network scales and ionospheric activity levels. First, the receiver-specific hardware delays are precisely calibrated for the maximum utilization of ionospheric corrections retrieved in PPP-RTK networks. Then, a grid-based polynomial fitting and residual interpolation model is developed with a stochastic model considering the distribution of reference stations, the elevation of satellites, and rate of total electron content index (ROTI). Three networks situated in different latitudes with the max inter-station distance of 26.7 km, 134.2 km, and 247.9 km, respectively, were employed to verify the enhancement to PPP-RTK. The proposed method presents a significant improvement in reducing the convergence time of PPP-RTK in all three networks, with the horizontal convergence time decreased from 5 to 14 s to less than 1 s in the small- and medium-scale networks, 44–25 s in the large-scale network compared to the modified linear combination method (MLCM). Besides, a vehicular experiment on an urban loop was conducted for further validation. The positioning accuracy of the PPP-RTK vehicular solutions with the newly proposed method is 2.74, 2.28 and 5.54 cm in the east, north and up components, respectively, with an improvement of 10, 11 and 40% over MLCM. The proportion of 3D positioning accuracy less than 5 cm also increased from 50.1 to 87.8%. Moreover, during the ionospheric active period, the average positioning accuracy is increased from decimeter- to centimeter-level horizontally, and the fixing rate can be increased from 80.6 to 90.0%.","tags":["GNSS","PPP-RTK"],"title":"A grid-based ionospheric weighted method for PPP-RTK with diverse network scales and ionospheric activity levels","type":"publication"},{"authors":["Chenjun Long","Zongzhou Wu","Zhiheng Shen"],"categories":null,"content":" ","date":1689033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689033600,"objectID":"2acf6414052b431b7eda167bb56d7e44","permalink":"https://zzwu29.github.io/publication/journal-article/pnt-2/","publishdate":"2023-07-11T00:00:00Z","relpermalink":"/publication/journal-article/pnt-2/","section":"publication","summary":"Global satellite navigation system(GNSS) can provide all-round and high-precision navigation, positioning and timing services around the world. Absolute positioning technology represented by precise point positioning(PPP) has attracted widespread attention due to its advantages of high accuracy, global consistency and flexible operating range. But the long convergence time limits its use in real-time and fast precision positioning applications. In order to solve the problems, an ultra-wideband(UWB) enhanced PPP method is proposed, which tightly integrates UWB and PPP to improve the positioning accuracy. The experimental results show that in the dynamic scenario, the proposed method reduces the root mean square(RMS) values of the PPP result of the GPS/GAL system by 76.99%, 21.46%,64.53%, and the GPS/GAL/BDS sytems by 69.69%, 37.21% and 61.32%,in east, north and up directions, with their convergence time being shortened by 62.78% and 57.75%, respectively. The evaluation of anchor points number, which also represents geometric configuration, shows that the RMS values of the 3D error of the GPS/GAL and GPS/GAL/BDS systems can be reduced by 67.98% and 59.35%, and the convergence time can be shortened by 76.14% and 62.68% with only 4 anchors, so as to achieve the enhancement effect of the best comprehensive cost and performance.","tags":["seamless positioning","GNSS"],"title":"UWB enhanced GNSS precise point positioning based on raw measurements","type":"publication"},{"authors":["Zhili Xu","Zhuohao Yan","Xingxing Li","Zhiheng Shen","Yuxuan Zhou","Zongzhou Wu","Xin Li"],"categories":null,"content":" ","date":1683763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683763200,"objectID":"b13fb5cccd53032f454d8d4fd8b6c969","permalink":"https://zzwu29.github.io/publication/journal-article/pnt-1/","publishdate":"2023-05-11T00:00:00Z","relpermalink":"/publication/journal-article/pnt-1/","section":"publication","summary":"With the efficiency improvement of industries in the information era, the traditional human-driven traffic system has gradually failed to meet people's demand for high-efficiency and low-risk traffic services, and the emerging intelligent driving technology has brought opportunities to this field. Nowadays, intelligent driving, represented by autonomous driving, has become a practical and deep crossing technology. Its core modules include high-precision positioning, scene perception, planning, control and other aspects. As the most basic and core functional module in an intelligent driving system, the positioning module needs to be of high-precision, high-availability, and low-latency performance. At present, the multi-sensor fusion technology integrated with high-precision satellite navigation, inertial navigation and environmental perception has become the recognized core means to realize ubiquitous intelligent driving, and accurate and reliable positioning services can be achieved by making full use of the measurement information of on-board sensors. Starting from the sensor technology commonly used in navigation and positioning, we comprehensively review the high-precision positioning technology involved in the current field of intelligent driving, present the mainstream multi-sensor fusion framework based on filtering and factor graph optimization and sort out representative algorithms. Finally, we summarize the current development status of high-precision positioning technology in intelligent driving and prospect the development trend in the future.","tags":["autonomous driving","sensor fusion"],"title":"Review of high-precision multi-sensor integrated positioning toward intelligent driving","type":"publication"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":"A custumized matplotlib style (.mplstyle) You can try this matplotlib style.\nUsage Install scienceplots package here, and copy the mplstyle file to its directory.\nimport scienceplots plt.style.use([\u0026#39;zongzhouwu\u0026#39;]) Useful links https://cdn.elifesciences.org/author-guide/tables-colour.pdf\nhttps://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html\nhttps://antv.antgroup.com/specification/language/palette\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\nhttps://matplotlib.org/stable/users/explain/colors/colormaps.html\nhttps://ggplot2.tidyverse.org/reference/#scales\nhttp://colorbrewer2.org/\nhttps://colorspace.r-forge.r-project.org/index.html\nhttps://github.com/garrettj403/SciencePlots\nhttps://github.com/mtennekes/cols4all\nhttps://github.com/coatless-rpkg/cetcolor\nhttps://materialui.co/\nhttp://brandcolors.net/\nhttps://moviesincolor.com/films\nhttps://personal.sron.nl/~pault/#sec:qualitative\nhttps://coolors.co/palettes/palettes\nhttps://www.webdesignrankings.com/resources/lolcolors/\nhttps://www.schemecolor.com/\nPalette example This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n","date":1665273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665273600,"objectID":"3c3a23b4164d6c1c28fb1ff205b413ee","permalink":"https://zzwu29.github.io/post/color-palettes/","publishdate":"2022-10-09T00:00:00Z","relpermalink":"/post/color-palettes/","section":"post","summary":"Color Palette Overview","tags":["color","palette","tool"],"title":"Palette","type":"post"},{"authors":null,"categories":null,"content":" ","date":1651017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651017600,"objectID":"4bed5d093885bec217ce3e10d35cb520","permalink":"https://zzwu29.github.io/project/comparenavplot/","publishdate":"2022-04-27T00:00:00Z","relpermalink":"/project/comparenavplot/","section":"project","summary":"PySide2 UI for plotting IMU outputs, GNSS or GNSS/INS positioning results","tags":["Tool"],"title":"compareNavPlot","type":"project"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":" Dataset Institution Year GNSS (Raw) IMU Camera LiDAR UWB Ground Truth Platform HKU MARS HKU 2023 √ (√) √ √ √ √ UAV STAR-loc UofT 2023 √ √ √ √ UAV S3E SYSU 2023 √ (×) √ √ √ √ 3×UGV GRACO SYSU 2023 √ (×) √ √ √ √ UAV/UGV Hilti Hilti 2023 √ √ √ √ Hand/UGV GVINS HKUST 2022 √ (√) √ √ √ Hand/Car VIRAL NTU 2022 √ √ √ √ √ UAV SJTU_GVI SJTU 2022 √ (√) √ √ √ Car UrbanNav PolyU 2022 √ (√) √ √ √ √ Car FusionPortable HKUST 2022 √ (×) √ √ √ √ Hand Marine Perception MIT 2022 √(×) √ √ √ √ Canoe Hilti Hilti 2022 √ √ √ √ Hand Insane University of Klagenfurt 2022 √(×) √ √ √ √ UAV Visual-Inertial-LiDAR Leibniz Universität Hannover 2022 √ √ √ Car VECtor ShanghaiTech University 2022 √ √ √ √ Hand M2DGR SJTU 2021 √ (√) √ √ √ √ UGV GroundRobot CAS 2021 √ (×) √ √ √ √ UGV Hilti Hilti 2021 √ √ √ √ Hand UTBM UTBM 2020 √ (×) √ √ √ √ Car UrbanLoco PolyU 2020 √ (√) √ √ √ √ Car TEX-CUP University of Texas Austin 2020 √(√) √ √ √ Car Kaist Urban KAIST 2019 √ (×) √ √ √ √ Car TUM VI TUM 2019 √ √ √ UGV FPV ETH 2019 √ √ √ UAV Canoe University of Illinois Board of Trustees 2018 √ √ √ Canoe MVSEC University of Pennsylvania 2018 √ √ √ √ Car/Motorbike/UAV/Hand Zurich MAV ETH 2017 √ (×) √ √ √ UAV Ford Ford 2017 √ (×) √ √ √ √ Car KITTI KIT 2016 √ (×) √ √ √ √ Car Robotcar Oxford 2016 √ (×) √ √ √ √ Car EuRoC ETH 2016 √ √ UAV Maplab ETH 2016 √ √ Hand ","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"b75db12682b362f7f50101da48d6ef80","permalink":"https://zzwu29.github.io/post/slam-datasets/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/post/slam-datasets/","section":"post","summary":"SLAM dataset with different sensor suite","tags":["dataset","SLAM"],"title":"SLAM Dataset Overview","type":"post"},{"authors":["Zongzhou Wu"],"categories":["research"],"content":"Introduction Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\nSeaborn website: https://seaborn.pydata.org/\nSeaborn Github: https://github.com/mwaskom/seaborn\nDependencies Seaborn supports Python 3.8+.\nInstallation requires numpy, pandas, and matplotlib. Some advanced statistical functionality requires scipy and/or statsmodels.\nInstallation pip install seaborn Object Interface In version 0.12, Seaborn introduced object namespace and interface, which are based on the Grammar of Graphics similiar to ggplot2 package in R. In the past, the best python package was plotnine that was absolutely consistent with ggplot2.\nExample Basic Plot import seaborn import seaborn.objects as so tips=seaborn.load_dataset(\u0026#34;tips\u0026#34;) ( so.Plot(tips, y=\u0026#34;day\u0026#34;, color=\u0026#34;sex\u0026#34;) .add(so.Bar(), so.Hist(), so.Dodge()) .show() ) Then figure will be like this\nIf you want to save the figure, just run\nimport seaborn as sns import seaborn.objects as so tips=sns.load_dataset(\u0026#34;tips\u0026#34;) ( so.Plot(tips, y=\u0026#34;day\u0026#34;, color=\u0026#34;sex\u0026#34;) .add(so.Bar(), so.Hist(), so.Dodge()) .save(\u0026#34;fig.pdf\u0026#34;,bbox_inches=\u0026#34;tight\u0026#34;) ## save ) bbox_inches=“tight” is necessary for saving legend like R.\nFacet import seaborn as sns import seaborn.objects as so penguins=sns.load_dataset(\u0026#34;penguins\u0026#34;) ( so.Plot(penguins, \u0026#34;bill_length_mm\u0026#34;, \u0026#34;bill_depth_mm\u0026#34;) .add(so.Dots()) .facet(\u0026#34;species\u0026#34;, \u0026#34;sex\u0026#34;) .show() ) Theme and Palette import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt sns.set_theme(style=\u0026#34;white\u0026#34;, palette=\u0026#34;deep6\u0026#34;, font=\u0026#34;Times New Roman\u0026#34;, font_scale=1.5) ## settings tips=sns.load_dataset(\u0026#34;tips\u0026#34;) ( so.Plot(tips, \u0026#34;total_bill\u0026#34;, \u0026#34;tip\u0026#34;, color=\u0026#34;day\u0026#34;) .facet(col=\u0026#34;day\u0026#34;) .add(so.Dot(color=\u0026#34;#aabc\u0026#34;), col=None, color=None) .add(so.Dot()) .theme(plt.rcParams) ## important .show() ) Reference https://seaborn.pydata.org/api.html#objects-api\nhttps://seaborn.pydata.org/tutorial/objects_interface.html\n","date":1635379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635379200,"objectID":"d5f91b5c4d79694895263e92406cb54d","permalink":"https://zzwu29.github.io/post/seaborn-objects/","publishdate":"2021-10-28T00:00:00Z","relpermalink":"/post/seaborn-objects/","section":"post","summary":"a advanced graphic grammar","tags":["tool"],"title":"Seaborn Objects","type":"post"},{"authors":["Zongzhou Wu"],"categories":["linux"],"content":"How to install nvidia driver on ubuntu 20.04 ? https://blog.csdn.net/qq_42887760/article/details/126903100?spm=1001.2014.3001.5506 https://blog.csdn.net/qq_38043069/article/details/128400761?spm=1001.2014.3001.5506 https://blog.csdn.net/wf19930209/article/details/81877822?spm=1001.2014.3001.5506\nLegion Y9000P bios: F2\nSetting discrete graphics\ntty: Ctrl + Alt + F(3/4/5……)\n","date":1620259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620259200,"objectID":"16bda3d69041c122754556a4766def49","permalink":"https://zzwu29.github.io/post/nvidia-driver/","publishdate":"2021-05-06T00:00:00Z","relpermalink":"/post/nvidia-driver/","section":"post","summary":"How to install nvidia driver on ubuntu 20.04","tags":["tool","driver"],"title":"Ubuntu Nvidia Driver","type":"post"}]